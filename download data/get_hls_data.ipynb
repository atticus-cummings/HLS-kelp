{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "import geopandas as gp\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from contextlib import redirect_stdout\n",
    "import sys\n",
    "if r'C:\\Users\\attic\\HLS Kelp Detection\\tools' not in sys.path:\n",
    "    sys.path.append(r'C:\\Users\\attic\\HLS Kelp Detection\\tools')\n",
    "import earth_access_tools as eat\n",
    "import csv\n",
    "import earthaccess\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import shutil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set time and location\n",
    "geojson_path = r'C:\\Users\\attic\\HLS Kelp Detection\\maps\\Isla_Vista_Kelp.geojson'\n",
    "field = gp.read_file(geojson_path)\n",
    "bbox = tuple(list(field.total_bounds))\n",
    "bbox #Display coordinate bounds\n",
    "with open(geojson_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "# Extract the name\n",
    "temporal = (\"2024-1-01T00:00:00\", \"2024-3-01T00:00:00\") #\n",
    "\n",
    "#DEM map doesn't work great as is. For processing, I recommend downloading one from: https://www.ncei.noaa.gov/maps/bathymetry/ and inserting it into the /imagery folder\n",
    "create_dem = False # set true if u want a digital elevation map \n",
    "dem_name = 'dem.tif'\n",
    "\n",
    "num_to_download = 5000 #set this value to the number of frames you want\n",
    "load_num = -1 #sets number of granules to load, this should generally be >> than num_download; -1 loads all \n",
    "\n",
    "download_to_path = r'C:\\Users\\attic\\HLS Kelp Detection\\imagery\\tiles'\n",
    "tiles = ['20FQJ']#['19QFV','19QGV','19QHV','20QLE', '20QMD','19PGT', '19PHT','20PLC']  \n",
    "clouds = 60\n",
    "threads = os.cpu_count() / 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eat.download_hls_data(temporal,tiles=tiles, dem_download=True, bbox=None, data_folder=download_to_path, num_download=num_to_download, load_num=load_num, cloud_coverage=clouds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping for potential future reference\n",
    "\n",
    "\n",
    "# folder_path = os.path.join(os.path.join(download_to_path,location))\n",
    "# ## ======= create location folder path ======= ##\n",
    "# if not os.path.isdir(folder_path):\n",
    "#     os.mkdir(folder_path)\n",
    "\n",
    "# ## ======= Create/write DEM, if requested ======= ##\n",
    "# if create_dem:\n",
    "#     if not os.path.isfile(os.path.join(folder_path, dem_name)):\n",
    "#             dem_path = earthaccess.download(dem_results[0], local_path=folder_path)\n",
    "#             os.rename(dem_path[0], os.path.join(folder_path,'dem.tif'))\n",
    "#             os.rename(dem_path[1], os.path.join(folder_path, 'num.tif'))\n",
    "\n",
    "# downloaded = 0\n",
    "\n",
    "# for i, granule in enumerate(results):\n",
    "#     metadata = eat.extract_metadata(granule)\n",
    "#     ## ======= Parse metadata ======= ##\n",
    "\n",
    "#     name = metadata['native-id']\n",
    "\n",
    "#     #For some reason, attributes are parsed into a list in the HLS metadata. This reformats it into a dictionary.\n",
    "#     #print(attributes['MGRS_TILE_ID'])\n",
    "\n",
    "#     if(int(metadata['CLOUD_COVERAGE']) > 50): #Reject granules with large cloud cover, for now\n",
    "#         continue\n",
    "#     time = metadata['SENSING_TIME']\n",
    "#     tile_folder = metadata['MGRS_TILE_ID']\n",
    "#     if specific_tile and not tile_folder == tile:\n",
    "#         continue\n",
    "#     ## ======= Create file directory, if needed  ======= ##\n",
    "#     tile_path = os.path.join(folder_path,tile_folder)\n",
    "#     if not os.path.isdir(tile_path):\n",
    "#          os.mkdir(tile_path)\n",
    "#     folder_name = (f'{name}')\n",
    "#     file_path = os.path.join(tile_path,folder_name)\n",
    "#     if not os.path.isdir(file_path):\n",
    "#         os.mkdir(file_path) #Make folder for granule \n",
    "\n",
    "#     # sorted data links \n",
    "#     all_links = granule.data_links()\n",
    "#     links = get_band_links(all_links, name)\n",
    "\n",
    "#     ## ======= download granule ======= ##\n",
    "#     with open(os.devnull, 'w') as f, redirect_stdout(f): #The print out of this is kind of annoying, this redirects *most* of it \n",
    "#         downloadPath = earthaccess.download(links, local_path=file_path, threads=2)\n",
    "#     downloaded = downloaded + 1\n",
    "#     print(f'{name}')\n",
    "#     if downloaded > num_to_download:\n",
    "#          break\n",
    "\n",
    "#     ## ======= write metadata csv ======= ##\n",
    "#     save_metadata_csv(metadata,file_path,folder_name)\n",
    "\n",
    "#Alternatively, this recompiles the downloaded data into a single tif. \n",
    "#The previous method is probably better. Leaving this just in case.\n",
    "\n",
    "# temp_path = r'C:\\Users\\attic\\HLS Kelp Detection\\temp'\n",
    "# folder_path = os.path.join(os.path.join(download_to_path,location))\n",
    "# ## ======= create location folder path ======= ##\n",
    "\n",
    "# downloaded = 0\n",
    "\n",
    "# for i, granule in enumerate(results):\n",
    "#     if os.path.isdir(temp_path):\n",
    "#        shutil.rmtree(temp_path)\n",
    "#     os.mkdir(temp_path)\n",
    "#     metadata = extract_metadata(granule)\n",
    "#     ## ======= Parse metadata ======= ##\n",
    "\n",
    "#     name = metadata['native-id']\n",
    "\n",
    "#     #For some reason, attributes are parsed into a list in the HLS metadata. This reformats it into a dictionary.\n",
    "#     #print(attributes['MGRS_TILE_ID'])\n",
    "\n",
    "#     if(int(metadata['CLOUD_COVERAGE']) > 50): #Reject granules with large cloud cover, for now\n",
    "#         continue\n",
    "#     time = metadata['SENSING_TIME']\n",
    "#     tile_folder = metadata['MGRS_TILE_ID']\n",
    "#     if specific_tile and not tile_folder == tile:\n",
    "#         continue\n",
    "#     ## ======= Create file directory, if needed  ======= ##\n",
    "#     tile_path = os.path.join(folder_path,tile_folder)\n",
    "#     if not os.path.isdir(tile_path):\n",
    "#          os.mkdir(tile_path)\n",
    "#     file_path = os.path.join(tile_path,f'{name}.tif')\n",
    "#     if os.path.isfile(file_path):\n",
    "#         continue\n",
    "#     # if not os.path.isdir(file_path):\n",
    "#     #     os.mkdir(file_path) #Make folder for granule \n",
    "\n",
    "#     # sorted data links \n",
    "#     all_links = granule.data_links()\n",
    "#     links = get_band_links(all_links, name)\n",
    "\n",
    "#     ## ======= download granule ======= ##\n",
    "#     #with open(os.devnull, 'w') as f, redirect_stdout(f): #The print out of this is kind of annoying, this redirects *most* of it \n",
    "#     downloadPath = earthaccess.download(links, local_path=temp_path)\n",
    "#     bands = []\n",
    "#     transform = None\n",
    "\n",
    "#     for file in get_sorted_files(folder_path=temp_path,granule_name=name):\n",
    "#         with rasterio.open(os.path.join(temp_path,file), 'r') as src:\n",
    "#             bands.append(src.read())\n",
    "#             if transform is None:\n",
    "#                 transform = src.transform\n",
    "#                 crs = src.crs\n",
    "\n",
    "#     bands = np.vstack(bands)\n",
    "#     print(bands.shape)\n",
    "#     num_bands, height, width = bands.shape\n",
    "#     data_type = rasterio.int16\n",
    "#     profile = {\n",
    "#         'driver': 'GTiff',\n",
    "#         'width': width,\n",
    "#         'height': height,\n",
    "#         'count': num_bands,  # one band  B02, B03, B04, and B05, classified, mesma (Blue, Green, Red, and NIR).\n",
    "#         'dtype': data_type,  # assuming binary mask, adjust dtype if needed\n",
    "#         'crs': crs,\n",
    "#         'transform': transform,\n",
    "#         'nodata': 0,  # assuming no data is 0\n",
    "#         'tags': metadata\n",
    "#     }\n",
    "\n",
    "#     with rasterio.open(file_path, 'w', **profile) as dst: #output file is a stack of 6 HLS bands (in chronological order) + Fmask as image 7\n",
    "#         for i,band in enumerate(bands):\n",
    "#             dst.write(band.astype(data_type), i + 1)\n",
    "\n",
    "    \n",
    "#     downloaded = downloaded + 1\n",
    "#     shutil.rmtree(temp_path)\n",
    "#     print(f'{name}')\n",
    "#     if downloaded > num_to_download:\n",
    "#          break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hls_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
